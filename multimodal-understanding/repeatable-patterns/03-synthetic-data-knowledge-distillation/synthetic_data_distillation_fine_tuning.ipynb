{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data and Distilling Knowledge to Fine-Tune Smaller Models with Amazon Nova Pro\n",
    "\n",
    "In this notebook, we will walk you through the process of utilizing a larger language model (LLM) like Amazon Nova Pro, Amazon's latest and most advanced model, to create a dataset for instruction fine-tuning. This dataset will be used to perform distillation by fine-tuning a smaller model, such as Amazon Nova Micro\n",
    "\n",
    "By leveraging the capabilities of Amazon Nova Pro, we can generate high-quality, concise training data that enhances the performance of smaller models. This approach is particularly useful for tasks that require detailed and specific instructions.\n",
    "\n",
    "Before we begin, ensure that you have access to Amazon Nova Pro, which is now available on Amazon Bedrock foundation models. You can find the dataset `deepmind/aqua_rat` we will be using [here](https://huggingface.co/datasets/deepmind/aqua_rat).\n",
    "\n",
    "You can run the notebook on an Amazon SageMaker Studio notebook, or a SageMaker notebook instance without manually setting your aws credentials.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "### Amazon Bedrock\n",
    "\n",
    "Amazon Bedrock is a fully managed service by AWS that simplifies the integration of generative AI into applications. It provides access to a variety of foundation models for tasks like natural language processing and image generation, offering APIs and SDKs for easy integration. Designed for scalability, security, and cost-effectiveness, Amazon Bedrock abstracts the complexities of deploying and managing AI models, allowing developers to focus on building their applications while leveraging AWS's robust infrastructure. This service is ideal for businesses looking to incorporate AI capabilities without requiring deep expertise in machine learning or infrastructure management.\n",
    "\n",
    "### Amazon Nova Pro Model\n",
    "\n",
    "Amazon Nova Pro is the largest model in the family of Amazon Nova models. Amazon Nova model family is a collection of pre-trained and instruction-tuned LLMs which already includes micro and lite sizes. Amazon Nova Pro comes with new capabilities including multi-language support and a 300k context window. These models are stronger overall capabilities and are ideal for content creation, conversational AI, language understanding, research and development (R&D), and enterprise applications.\n",
    "\n",
    "\n",
    "### Amazon Nova Micro Model\n",
    "\n",
    "Amazon Nova Micro Model is a low latency and cost-effective, text-to-text LLM. The context length is 128k tokens, and max output is 5K. This LLM is designed to deliver high performance across a variety of tasks while maintaining cost efficiency. This model is particularly advantageous for developers and organizations looking to implement advanced AI capabilities without the need for extensive computational resources. \n",
    "\n",
    "\n",
    "### Prequisites\n",
    " In order to follow along in this notebook, you'll need access to the following:\n",
    "\n",
    " - An AWS account with Amazon Bedrock model access to Amazon's frontier models. See the model access page [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html).\n",
    "\n",
    " - An [AWS Identity and Access Management (IAM)](https://aws.amazon.com/iam/) role to access Amazon Bedrock models. To learn more about how IAM works with Amazon Bedrock, refer to [Identity and Access Management for Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-prereq.html).\n",
    " \n",
    " - Access to SageMaker Studio or a SageMaker notebook instance or an interactive development environment (IDE) such as PyCharm or Visual Studio Code. We recommend using SageMaker Studio for straightforward deployment and inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we perform the following high level steps: \n",
    "\n",
    "1. **Evaluate Initial Performance**: We explore and prepare questions and answer dataset. Then we assess the quality and performance of sample inference responses from Amazon Nova Pro and Micro models using the deepmind/aqua_rat dataset.\n",
    "\n",
    "2. **Generate Synthetic Training Data**: We create synthetic training data with Amazon Nova Pro, which serves as the teacher model.\n",
    "\n",
    "3. **Perform Knowledge Distillation**: We first transform systhetic data in the training dataset format needed to fine-tune Amazon Nova model. We then fine-tune Amazon Nova Micro using the synthetic training dataset generated by Amazon Nova Pro.\n",
    "\n",
    "4. **Test the Fine-Tuned Model**: We deploy the the fine tuned model Amazon Nova Micro using Bedrock's Provisioned Throuput feature, and evaluate the fine-tuned model by testing it against the test dataset to demonstrate the improvement in response quality when compared against base model.\n",
    "\n",
    "5. **Conclusion**: The notebook concludes with a summary of the knowledge distillation process and highlights how the teacher model's expertise is effectively transferred to the smaller student model. \n",
    "\n",
    "6. **Cleanup**: At the end of the notebook, we provide guidance on cleaning a fine-tuned custom model artifact, and Bedrock's Provisioned Throughput capacity used to deploy the fine-tuned custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Evaluate Initial Performance\n",
    "We explore and prepare questions and answer dataset. Then we assess the quality and performance of sample inference responses from Amazon Nova Pro and Micro models using the deepmind/aqua_rat dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Dataset Exploration and Preparation\n",
    "\n",
    "In this section, we will explore a dataset from the Hugging Face Hub using the HF Datasets library. Hugging Face provides a vast collection of datasets for various tasks in natural language processing (NLP), computer vision, and audio processing. This exploration will help us understand the structure, features, and contents of the dataset, enabling us to prepare it for training and evaluation in our machine learning models. The [deepmind/aqua_rat](https://huggingface.co/datasets/deepmind/aqua_rat) dataset is a large-scale collection of approximately 100,000 algebraic word problems, each accompanied by a detailed natural language rationale explaining the solution process. This dataset is designed to train and evaluate models that not only generate the correct answer but also provide a step-by-step explanation, making it ideal for tasks requiring mathematical reasoning and natural language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the dataset\n",
    "dataset_name = \"deepmind/aqua_rat\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Split the train dataset into train and test splits\n",
    "split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "train_test_split = dataset['train'].train_test_split(test_size=1 - split_ratio)\n",
    "\n",
    "# Optionally split the train set further into train and validation sets\n",
    "train_validation_split = train_test_split['train'].train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "# Create a new dataset dictionary with the new splits\n",
    "dataset = DatasetDict({\n",
    "    'train': train_validation_split['train'],\n",
    "    'validation': train_validation_split['test'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "# Display the number of examples in each split after splitting\n",
    "print(\"\\nNumber of Examples in Each Split After Splitting:\")\n",
    "for split in dataset.keys():\n",
    "    if dataset[split] is not None:\n",
    "        print(f\"{split}: {len(dataset[split])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(dataset)\n",
    "\n",
    "# Display the dataset's features\n",
    "print(\"\\nDataset Features:\")\n",
    "print(dataset['train'].features)\n",
    "\n",
    "# Display a few examples from the dataset\n",
    "print(\"\\nSample Examples:\")\n",
    "for i in range(3):\n",
    "    print(dataset['train'][i])\n",
    "\n",
    "# Extract 20 questions from the train split\n",
    "questions = dataset['train'].select(range(20))['question']\n",
    "\n",
    "# Display the first 20 questions\n",
    "print(\"\\nFirst 20 Questions:\")\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"{i+1}: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Quality of response before fine-tuning Amazon Nova Micro\n",
    "\n",
    "In this section, we will review response from Amazon Nova Micros as is. And, later we will use this output response to compare how knwoledge distillation helped improve quality of Amazon Nova Micro. In addition to reviewing the quality, note `latencyMs` metric to see how smaller models are faster than larger models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = questions[1]\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b.1 Compare Responses of Amazon Nova Micro and Pro models with a sample question\n",
    "\n",
    "***\n",
    "`Amazon Nova Micro`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create a Bedrock Runtime client\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "MODEL_ID = \"amazon.nova-micro-v1:0\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system = [\n",
    "    {\n",
    "        \"text\": \"You are good at tasks requiring mathematical reasoning and natural language understanding. Start your response with a correct answer, and provide the rationale behind your answer\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Your user prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": question}]},\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"maxTokens\": 500, \"topP\": 0.1, \"temperature\": 0.3}\n",
    "\n",
    "model_response = client.converse(modelId=MODEL_ID, messages=messages, system=system, inferenceConfig=inf_params)\n",
    "\n",
    "print(\"\\n[Full Response]\")\n",
    "print(json.dumps(model_response, indent=2))\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(model_response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`Amazon Nova Pro`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create a Bedrock Runtime client\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system = [\n",
    "    {\n",
    "        \"text\": \"You are good at tasks requiring mathematical reasoning and natural language understanding. Start your response with a correct answer, and provide the rationale behind your answer\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Your user prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": question}]},\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"maxTokens\": 500, \"topP\": 0.1, \"temperature\": 0.3}\n",
    "\n",
    "model_response = client.converse(modelId=MODEL_ID, messages=messages, system=system, inferenceConfig=inf_params)\n",
    "\n",
    "print(\"\\n[Full Response]\")\n",
    "print(json.dumps(model_response, indent=2))\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(model_response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 2: Generate Synthetic Training Data\n",
    "In this section, we will leverage the Amazon Nova Pro model to generate high-quality synthetic data for distillation by fine-tuning the Amazon Nova Micro model. By using the Pro model to generate responses to domain-specific prompts, we can create a labeled dataset that will be used to fine-tune the Micro model, improving its accuracy and effectiveness in specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "config = Config(read_timeout=5000)\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\", config=config)\n",
    "\n",
    "# Set the model ID, e.g.,amazon.Nova-pro-v1:0\n",
    "model_id = \"us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "# Load the dataset and select the first 20 questions\n",
    "dataset = load_dataset('deepmind/aqua_rat', split='train')\n",
    "questions = dataset.select(range(500))['question']\n",
    "\n",
    "\n",
    "# Function to run inference and generate synthetic data using Bedrock\n",
    "def generate_synthetic_data(client, model_id, questions):\n",
    "    synthetic_data = []\n",
    "    for question in questions:\n",
    "        # Add Chain of Thought Reasoning prompt to the question\n",
    "        system = [\n",
    "           {\n",
    "               \"text\": \"You are good at tasks requiring mathematical reasoning and natural language understanding. Start your response with a correct answer, and provide the rationale behind your answer\"\n",
    "           }\n",
    "        ]\n",
    "        user_message = f\"{question}\"\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user_message}],\n",
    "            }\n",
    "        ]\n",
    "        try:\n",
    "            # Send the message to the model, using a basic inference configuration.\n",
    "            response = client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=conversation,\n",
    "                system=system,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": 500,\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"topP\": 0.1\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Extract the response text\n",
    "            response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "            synthetic_data.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": response_text\n",
    "            })\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "            break\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate synthetic data using Amazon Nova Pro (for 500 records, it takes 50 mins)\n",
    "synthetic_data = generate_synthetic_data(client, model_id, questions)\n",
    "\n",
    "# Save the synthetic data to a JSONL file\n",
    "with open('synthetic_data.jsonl', 'w') as f:\n",
    "    for entry in synthetic_data:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Perform Knowledge Distillation\n",
    "We first transform systhetic data in the training dataset format needed to fine-tune Amazon Nova model. We then fine-tune Amazon Nova Micro using the synthetic training dataset generated by Amazon Nova Pro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.a Transform the jsonl output to Nova model's fine-tuning format\n",
    "\n",
    "In this section, we will convert the jsonl format to match with schema needed for fine-tuning Amazon Nova Micro model. This is typical schema needed for each json line:\n",
    "json\n",
    "```\n",
    "{\n",
    "    \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "    \"system\": [\n",
    "      {\n",
    "        \"text\": \"You are good at tasks requiring mathematical reasoning and natural language understanding. Start your response with a correct answer, and provide the rationale behind your answer\"\n",
    "      }\n",
    "    ],\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\":\"{question}\"\n",
    "          }\n",
    "        ]},\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\": \"{answer}\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "def load_template(template_path):\n",
    "    with open(template_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def transform_data(jsonl_path, template, output_path):\n",
    "    with open(jsonl_path, 'r') as input_file, open(output_path, 'w') as output_file:\n",
    "        for line_number, line in enumerate(input_file, 1):\n",
    "            try:\n",
    "                # Parse each line as JSON\n",
    "                record = json.loads(line.strip())\n",
    "                \n",
    "                # Create a deep copy of the template to avoid modifying the original\n",
    "                transformed_record = copy.deepcopy(template)\n",
    "                \n",
    "                # Update the instruction and response in the messages array\n",
    "                for message in transformed_record['messages']:\n",
    "                    if message['role'] == 'user':\n",
    "                        message['content'][0]['text'] = record['question']\n",
    "                    elif message['role'] == 'assistant':\n",
    "                        message['content'][0]['text'] = record['answer']\n",
    "                \n",
    "                # Write the transformed record to the output file\n",
    "                output_file.write(json.dumps(transformed_record) + '\\n')\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error processing line {line_number}: Invalid JSON - {e}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Error processing line {line_number}: Missing field '{e}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line_number}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load the template\n",
    "        template = load_template('template.json')\n",
    "        \n",
    "        # Process the files\n",
    "        transform_data(\n",
    "            jsonl_path='synthetic_data.jsonl',\n",
    "            template=template,\n",
    "            output_path='train.jsonl'\n",
    "        )\n",
    "        print(\"Transformation complete!\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid template JSON - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.b Upload Files to S3 for Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'synthetic-generated-data-<uniqueID>'  # Create a new bucket or use an existing one\n",
    "subdirectory = 'amazon-nova-pro'\n",
    "train_data_location = f\"s3://{bucket_name}/{subdirectory}\"\n",
    "\n",
    "files_to_upload = ['train.jsonl']\n",
    "\n",
    "# Upload the files to the specified subdirectory\n",
    "for file_name in files_to_upload:\n",
    "    file_path = file_name  # File is in the same directory as the notebook\n",
    "    key_path = f\"{subdirectory}/{file_name}\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"No such file or directory: '{file_path}'\")\n",
    "    \n",
    "    # Upload the file\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, key_path)\n",
    "        print(f\"File {file_name} uploaded successfully to {key_path}.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.b Fine-tuning Amazon Nova Micro\n",
    "\n",
    "In this section, we will dive deep into the process of distillation by fine-tuning the Amazon Nova Mirco model to enhance its performance for specific tasks. Fine-tuning involves training the pre-trained model on custom datasets to adapt it to particular domains or applications. Amazon Bedrock provides serverless fine-tuning capabilities, where you need provide the data set and the based model-id to train the model. \n",
    "\n",
    "#### 3.b.1 Pre-requisites\n",
    "1. Create or use an IAM Role that allows Amazon Bedrock fine-tuning job to access your training data. The IAM policy for this role \"Nova-fine-tuning-role\" looks like:\n",
    "json\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::synthetic-generated-data-<uniqueID>\",\n",
    "                \"arn:aws:s3:::synthetic-generated-data-<uniqueID>/*\"\n",
    "            ],\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:PrincipalAccount\": \"<aws-account-id>\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "2. And, allows Amazon Bedrock fine-tuning job to assume this role by adding the \"Trust relationship policy\" as mentioned below:\n",
    "json\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": \"<aws-account-id>\"\n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:us-east-1:<aws-account-id>:model-customization-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "3. Attach IAM:PassRole, and IAM:GetRole, to SageMaker execution role. The policy looks like:\n",
    "json\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor0\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:GetRole\",\n",
    "            \"Resource\": \"arn:aws:iam::<aws-account-id>:role/service-role/Nova-fine-tuning-role\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"arn:aws:iam::<aws-account-id>:role/service-role/Nova-fine-tuning-role\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b.2 Import libraries\n",
    "Here we are importing necessary libraries and modules. The code is initializing a Sagemaker session and a Bedrock client to interact with Amazon Sagemaker and Bedrock services via APIs respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import Session\n",
    "import uuid  # Import the 'uuid' module for generating a unique identifier\n",
    "\n",
    "# Create a session using the provided AWS SDK sessions\n",
    "session = Session(boto_session=boto3.session.Session(),\n",
    "                sagemaker_client=boto3.client('sagemaker'),\n",
    "                sagemaker_runtime_client=boto3.client('runtime.sagemaker'))\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock = boto3.client(service_name='bedrock', region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b.3 Set parameters for the training job\n",
    "Retrieve the Bedrock's IAM Execution Role and its ARN.\n",
    "\n",
    "Here we are initializing an IAM client. Information about the Bedrock IAM role is retrieved and the role ARN is saved to create a customization job on Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='iam')\n",
    "response = client.get_role(\n",
    "    RoleName='Nova-fine-tuning-role'\n",
    ")\n",
    "\n",
    "roleArn = response[\"Role\"][\"Arn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are configuring the necessary parameters like setting the base model id, hyperparameters, training data and model output S3 locations to initialize a Bedrock model customization job that will fine-tune the Amazon Nova Micro model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base model to use\n",
    "basemodelId = 'us.amazon.nova-micro-v1:0'\n",
    "job_prefix = \"customNova\"\n",
    "\n",
    "# Model ID for provisioned throughput\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-api.html\n",
    "baseModelIdentifierForProvisonedThroughput = \"arn:aws:bedrock:us-east-1::foundation-model/us.amazon.nova-micro-v1:0\"\n",
    "# Generate a unique identifier for the job and custom model name\n",
    "job_uuid = str(uuid.uuid4())[:8]  # Extracting the first 8 characters for brevity\n",
    "jobName = f\"{job_prefix}-{job_uuid}\"\n",
    "customModelName = f\"{job_prefix}-{job_uuid}\"\n",
    "\n",
    "hyperParameters = {\n",
    "    \"epochCount\": \"5\", #defines the number of times the training data is passed through the model during training\n",
    "    \"batchSize\": \"1\", #how many samples to work through before updating the internal model parameters\n",
    "    \"learningRate\": \"0.00001\", #defines how aggressively to update the model with each batch of data\n",
    "}\n",
    "\n",
    "# Retrieve the default bucket name from the session\n",
    "default_bucket =\"synthetic-generated-data-<uniqueID>\"\n",
    "\n",
    "# Specify the training data configuration using the previously uploaded S3 data\n",
    "s3_train_data = f\"s3://{default_bucket}/amazon-nova-pro/train.jsonl\"\n",
    "trainingDataConfig = {\"s3Uri\": s3_train_data}\n",
    "\n",
    "# Specify the output data configuration for the custom model\n",
    "outputDataConfig = {\"s3Uri\": f\"s3://{default_bucket}/fine-tuning-output/\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b.4 Trigger the bedrock training job\n",
    "We create a fine-tuning job using the Bedrock client. Once created, the job identifier is printed. This identifier can be used to track the status and results of the fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a job for model customization\n",
    "jobIdentifier = bedrock.create_model_customization_job(\n",
    "    jobName = jobName,\n",
    "    customizationType = \"FINE_TUNING\",\n",
    "    customModelName = customModelName,\n",
    "    roleArn = roleArn,\n",
    "    baseModelIdentifier = baseModelIdentifierForProvisonedThroughput,\n",
    "    hyperParameters = hyperParameters,\n",
    "    trainingDataConfig = trainingDataConfig,\n",
    "    outputDataConfig = outputDataConfig\n",
    ")\n",
    "\n",
    "# Print the identifier for the created job\n",
    "print(f\"Model customization job created with identifier: {jobIdentifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b.5 Monitor the job till the status is shown as \"Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fine_tune_job = bedrock.get_model_customization_job(jobIdentifier=jobIdentifier['jobArn'])\n",
    "print(fine_tune_job['status'])\n",
    "# The job may take more than an hour to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Testing the Amazon Nova Micro Fine-tuned Model \n",
    "\n",
    "In this section, we will evaluate the performance of the fine-tuned Amazon Nova Micro model to determine how well it has adapted to the specific tasks for which it was trained. Testing involves comparing the model's responses to a set of predefined questions or tasks against the baseline performance of the original, pre-trained model. This process helps us understand the improvements achieved through distillation by fine-tuning and identify any remaining areas for enhancement. By systematically examining the model's outputs, we can ensure that the fine-tuning process has effectively tailored the model to meet our specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a Create provisioned no-commit throughput for the custom model \n",
    "**(only run the following once the status of the above job is shown as \"Completed\")**\n",
    "\n",
    "This code is configuring the provisioned inference capacity for the custom model resulting from the fine-tuning job, so it can be deployed as a Amazon Bedrock managed endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customModelId=fine_tune_job['outputModelArn']\n",
    "\n",
    "provisionedModelName = f\"{job_prefix}-provisioned-{job_uuid}\"\n",
    "\n",
    "# Create the provisioned capacity without passing any commitment option\n",
    "provisionedModelArn = bedrock.create_provisioned_model_throughput(\n",
    "    modelUnits=1,\n",
    "    provisionedModelName=provisionedModelName, \n",
    "    modelId=customModelId\n",
    "   )['provisionedModelArn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.a.1 Check the provisoned capacity creation status\n",
    "This process will take a few minutes to complete. Ensure the status returns to \"InService\" before going to next section \"**Prepare the inference request**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Provisioned model status until it says \"InService\"\n",
    "provisionedModelStatus = bedrock.get_provisioned_model_throughput(provisionedModelId=provisionedModelArn)\n",
    "print (provisionedModelStatus['status'])\n",
    "# The process may take more than an hour to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print ( f\"provisionedModelArn = {provisionedModelArn}\")\n",
    "print ( f\"customModelId = {customModelId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b Prepare sample dataset for the inference request\n",
    "Let's prepare a few inference request that will be sent to the fine-tuned custom model endpoint provisioned through Amazon Bedrock. The model will process the input text and return a generated summary based on the configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the dataset\n",
    "dataset_name = \"deepmind/aqua_rat\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Split the train dataset into train and test splits\n",
    "split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "train_test_split = dataset['train'].train_test_split(test_size=1 - split_ratio)\n",
    "\n",
    "# Optionally split the train set further into train and validation sets\n",
    "train_validation_split = train_test_split['train'].train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "# Create a new dataset dictionary with the new splits\n",
    "dataset = DatasetDict({\n",
    "    'train': train_validation_split['train'],\n",
    "    'validation': train_validation_split['test'],\n",
    "    'test': train_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract 8 questions, and their correct rationale from the dataset\n",
    "num_questions = 8\n",
    "questions = dataset['test'].select(range(num_questions))['question']\n",
    "answers = dataset['test'].select(range(num_questions))['rationale']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c Send the inference request to both base and fine-tuned custom model\n",
    "In the below cell, we will send inference request simultaneosly to both base model, and fine-tune custom model that has distilled knowledge from Amazon Nova Pro's synthetically generated  training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create the Bedrock Runtime client\n",
    "client = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'  # specify your region\n",
    ")\n",
    "\n",
    "#fine-tuned model_id and base model_id\n",
    "custom_model_id = provisionedModelArn\n",
    "base_model_id = 'us.amazon.nova-micro-v1:0'\n",
    "base_pro_model_id = 'us.amazon.nova-pro-v1:0'\n",
    "\n",
    "\n",
    "# Function to run inference on models using \"test\" dataset\n",
    "def generate_inference_data(client, model_id, questions):\n",
    "    inference_data = []\n",
    "    for question in questions:\n",
    "        # Add Chain of Thought Reasoning prompt to the question        \n",
    "        user_message = f\"{question}\"\n",
    "        system = [\n",
    "            {\"text\": \"You are good at tasks requiring mathematical reasoning and natural language understanding. Start your response with a correct answer, and provide the rationale behind your answer\"\n",
    "            }\n",
    "        ]\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user_message}],\n",
    "            }\n",
    "        ]\n",
    "        try:\n",
    "            # Send the message to the model, using a basic inference configuration.\n",
    "            response = client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=conversation,\n",
    "                system=system,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": 500,\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"topP\": 0.1\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Extract the response text\n",
    "            response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "            inference_data.append({\n",
    "                \"response\": response_text\n",
    "            })\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "            break\n",
    "\n",
    "    return inference_data\n",
    "\n",
    "# Run inference against fine-tuned custom model, base, and pro models using sample \"test\" dataset\n",
    "results_fine_tuned_nova_micro = generate_inference_data(client,custom_model_id,questions)\n",
    "results_base_nova_micro = generate_inference_data(client,base_model_id,questions)\n",
    "results_base_nova_pro = generate_inference_data(client,base_pro_model_id,questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.d Render the response against Test dataset and inference data from both base and custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a table of the outputs using HTML\n",
    "table_html = \"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Question</th>\n",
    "        <th>Dataset Answer</th>\n",
    "        <th>Fine-tuned Nova Micro Output</th>\n",
    "        <th>Nova Micro Output</th>\n",
    "        <th>Nova Pro Output</th>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "\n",
    "for i in range(8):\n",
    "    table_html += f\"\"\"\n",
    "    <tr>\n",
    "        <td>{questions[i]}</td>\n",
    "        <td>{answers[i]}</td>\n",
    "        <td>{results_fine_tuned_nova_micro[i]}</td>\n",
    "        <td>{results_base_nova_micro[i]}</td>\n",
    "        <td>{results_base_nova_pro[i]}</td>\n",
    "    </tr>\n",
    "    \"\"\"\n",
    "\n",
    "# Display the table using HTML\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Conclusion\n",
    "\n",
    "In this notebook, we have successfully demonstrated the process of distillation by fine-tuning and evaluating the Amazon Nova Micro model using Amazon Bedrock. By leveraging the advanced capabilities of the Amazon Nova Pro model, we generated high-quality synthetic data that served as a foundation for fine-tuning the smaller Nova Micro model. This approach allowed us to enhance the performance of the Nova Micro model, tailoring it to specific domain tasks and improving its accuracy and effectiveness.\n",
    "\n",
    "### 5.a Key Steps Accomplished:\n",
    "1. **Dataset Exploration**: We explored a sample dataset to understand its structure and contents, preparing it for use in model training and evaluation.\n",
    "2. **Data Generation with Amazon Nova Pro**: Utilizing the Amazon Nova Pro model, we generated synthetic data that provided high-quality responses to domain-specific prompts.\n",
    "3. **Distillation by Amazon Nova Micro**: We fine-tuned the Amazon Nova Micro model using the synthetic data, adapting it to better handle specific tasks and improving its overall performance.\n",
    "4. **Model Testing**: We tested the fine-tuned model against a set of evaluation questions, comparing its responses to those of the pre-trained model and assessing the improvements achieved through distillation by fine-tuning.\n",
    "\n",
    "### 5.b Results and Insights:\n",
    "- **Enhanced Performance**: The fine-tuned Amazon Nova Micro model demonstrated significant improvements in generating accurate and contextually relevant responses, showcasing the effectiveness of the fine-tuning process.\n",
    "- **Cost-Effective Adaptation**: By fine-tuning the smaller Amazon Nova Micro model with data generated from the larger Amazon Nova Pro model, we achieved high performance without the need for extensive computational resources, highlighting a cost-effective approach to model adaptation.\n",
    "- **Scalability and Flexibility**: The workflow outlined in this notebook can be scaled and adapted to various domains and tasks, providing a flexible framework for enhancing the capabilities of language models.\n",
    "\n",
    "### 5.c Future Work:\n",
    "- **Further Fine-Tuning**: Additional fine-tuning with more diverse and extensive datasets can further improve the model's performance and adaptability to different domains.\n",
    "- **Real-World Applications**: Deploying the fine-tuned model in real-world applications such as customer support, content generation, and domain-specific research can provide valuable insights and practical benefits.\n",
    "- **Continuous Evaluation**: Ongoing evaluation and monitoring of the model's performance will ensure that it remains effective and relevant as new data and requirements emerge.\n",
    "\n",
    "In conclusion, this notebook has provided a comprehensive guide to generate synthetic data using Amazon Nova Pro and use the generated data for distillation by fine-tuning and evaluating the Amazon Nova Micro model, demonstrating the potential of using advanced language models to address specific domain needs. By the steps outlined, practitioners can enhance their models' performance, achieve cost-effective adaptations, and unlock new possibilities in natural language processing and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 6: Delete the provisioned capacity and the custom model\n",
    "\n",
    "### 6.a Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock = boto3.client(service_name='bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.b Update the provisionedModelArn & customModelId values from the fine-tuning section above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the provisionedModelArn from section 4.a.1\n",
    "provisionedModelArn = \"<Update the value from the fine-tuning notebook>\"\n",
    "\n",
    "# Using the customModelId from section 4.a.1\n",
    "customModelId = \"<Update the value from the fine-tuning notebook>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.c Delete the provisioned capacity & the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the provisioned capacity\n",
    "bedrock.delete_provisioned_model_throughput(provisionedModelId=provisionedModelArn)\n",
    "\n",
    "# Delete the custom model\n",
    "bedrock.delete_custom_model(modelIdentifier=customModelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
